import { List } from "../../components/list";
import { ListItem } from "../../components/list-item";
import { Br } from "../../components/typography/br";

export function StateActorDrivenSabotage() {
  return (
    <>
      <h2>State actor driven sabotage</h2>
      <p>
        Though not a major highlight of this research, it is necessary to touch
        the topic of state actor driven sabotage against AI. This chapter shall
        correlate the topic of superhuman intelligence and its consequences as
        well as the nuclear age.
        <Br />
        Doing so, it is important that the concept of superhuman intelligence,
        or simply intelligence for that matter, has to be seen highly
        critically, yet it drives the current debate and thereby delivers
        proposals of state driven sabotage. Dan McQuillan summarises the concept
        of intelligence fittingly: “The idea of superior intelligence always
        comes with its corollary of inferior intelligence, [...] assessing some
        lives as more worthy than others.” (Dan McQuillan, 2025) The achieval of
        superhuman intelligence resembles a similar power position to the
        invention of nuclear weaponry. As we find ourselves in the second
        nuclear age, as defined by Paul Bracken (2013), simple nuclear
        deterrence has grown more complex due to the spread of nuclear ambitions
        and their realisations in a multitude of states, particularly in asia.
        <Br />
        Additionally the fear of unilateral power, as it was firstly achieved by
        the U.S through their completion of the nuclear bomb, now is mirrored in
        the domain of Artificial Intelligence. State actors are racing to fund
        development of ever increasingly intelligent AI models, ultimately
        trying to achieve superhuman intelligence.
        <Br />
        The superhuman intelligence report, written by major protagonists of the
        U.S’s tech-scene details a new concept detailing those novel
        geopolitical challenges. They introduce a concept named MAIM, closely
        resembling the more well known concept of MAM, which still regulates the
        use of nuclear weaponry through fear of self-obliteration. MAIN
        highlights the need of sabotage among state actors, should one come
        close to developing superhuman intelligence. The form of sabotage is
        vaguely described and increases in harm. This concept is promoted as
        assuring a function of stability, if in use. Yet this approach has to be
        viewed under extreme scrutiny. For one does Dan Hendrycks, one of the
        three authors, call for a political alignment of natural language
        models: “[arguing] after the last election perhaps [LLMs] should be
        biased toward Trump slightly, ‘because he won the popular vote.’” (Will
        Knight, 2025) Thus effectively amplifying outputs that have already been
        colored by bias even further towards more conservative/ far-right
        leaning opinions. Agency and one’s opinion building might thereby be
        even more condemned.
        <Br />
        In the second place, the proposal of MAIM misunderstands the
        similarities between the development of atomic intelligence and
        superhuman intelligence. Developing large scale Artificial Intelligence
        models does not require secure raw materials, such as enriched uranium.
        It is simply dependent on greatly capable GPU systems. Additionally
        development processes are marked in secrecy. It might not be feasible to
        conduct counter checks in order to keep track of one's opponents
        development of AI. This subtle shift might move the point of
        interference from needed to preemptive, potentially causing a domino
        effect of geopolitical consequences.
      </p>
      <h3>Sabotage methods and techniques to be deployed</h3>
      <p>
        Having pointed out the bias that may have been painted onto the report
        of the superintelligence strategy, it is still relevant to look closely
        at the methods of sabotage that are proposed, as this gives an idea of
        the capabilities of such techniques if they are subsidised and supported
        by state actors.
        <Br />
        The authors lay out a structured escalation ladder of sabotage. It
        ranges from covert degradation to full scale kinetic strikes. The least
        aggressive measures detail espionage and simple software attacks.
        Intelligence agencies therefore might rely on zero day exploits of
        common workplace tools, enabling them to gather insight of training runs
        and development. Once the advisory projects parameters are known, more
        direct sabotage is conceivable →
      </p>
      <List>
        <ListItem index={"poisoning of training datasets"}> </ListItem>
        <ListItem index={"introduction of faulty model weights"}> </ListItem>
        <ListItem index={"corruption of gradients during training"}> </ListItem>
      </List>
      <p>
        <Br />
        Those manipulations are hard to recognise as AI models are not outcome
        oriented. At the top of the escalation ladder are kinetic strikes. These
        employ the same logic as targeting missile silos during the Cold War,
        except the target is now a data centre. The authors justify this on the
        basis that if data centres are built in remote locations, “that action
        would not put cities into the crossfire” (Hendrycks et al., 2025). The
        chilling aspect of this logic lies in its normalisation. MAIM is
        introduced as a framework for stability: ”MAIM might be used to create a
        stable deterrence regime, preventing mutually assured AI destruction
        from escalating into mutual assured human destruction.” (Hendrycks et
        al., 2025). However, the authors have not reflected this as a method
        that requires transparency and rationality. Both of these qualities are
        rare, yet the consequences of an escalating war on superintelligence are
        grim.
        <Br />
        The report shows that sabotage will become a disruptive tool not only
        conducted by individuals, but also by state actors, thus occurring on a
        much larger scale. In this framing, sabotage is presented as inevitable
        and as a necessary tool. However, once deployed, the MAIM strategy may
        not guarantee stability, as the development of AI is much more difficult
        to observe than nuclear production.
      </p>
    </>
  );
}
